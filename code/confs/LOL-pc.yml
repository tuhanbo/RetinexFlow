
#python code/train.py --opt code/confs/LOL-pc.yml
#python code/test.py --opt code/confs/LOL-pc.yml

#### general settings
name: Retinex_flow_train_on_LOLv1
use_tb_logger: true
model: RetinexFlow
distortion: sr
scale: 1
gpu_ids: [0]
dataset: LoL
optimize_all_z: false
cond_encoder: ConEncoder_Retinex
train_with_gt_R: 0.1  #or lr_R

to_yuv: false
le_curve: false

#### datasets
datasets:
  train:
    root: /data1/zyyue/dataset/LOLv1
    quant: 32
    use_shuffle: true
    n_workers: 1  # per GPU
    batch_size: 32
    use_flip: true
    color: RGB
    use_crop: false
    GT_size: 160 # 192
    noise_prob: 0
    noise_level: 5
    log_low: false
    gamma_aug: false
    ### -------------Progressive training--------------------------
    batch_size_per_gpu: 32
    mini_batch_sizes: [ 32,20,16,8,4,4 ]   #[ 16,10,8,4,2,2 ]         # Batch size per gpu
    iters: [ 18400,12800,9600,7200,7200,4800 ]  #[ 9200,6400,4800,3600,3600,2400 ]
    gt_size: 384   # Max patch size for progressive training
    gt_sizes: [ 128,160,192,256,320,384 ]  # Patch sizes for progressive training.
    ### ------------------------------------------------------------

  val:
    root: /data1/zyyue/dataset/LOLv1
    n_workers: 1
    quant: 32
    n_max: 20
    batch_size: 1 # must be 1
    log_low: false
    use_crop_edge : true # need to crop edge to fit our model

#### Test Settings
dataroot_unpaired: /home/zyyue/ExDark Dataset/images/Car
dataroot_GT: /data1/zyyue/dataset/LOLv1/eval15/high
dataroot_LR: /data1/zyyue/dataset/LOLv1/eval15/low
model_path: pretrained_model/train_on_LOLv1.pth

heat: 0 # This is the standard deviation of the latent vectors

#### network structures
network_G:
  which_model_G: RetinexFlow

  flow:
    K: 12 #
    L: 3 # 4
    noInitialInj: true
    coupling: CondAffineSeparatedAndCond
    additionalFlowNoAffine: 2
    split:
      enable: false
    fea_up0: true
    conditionInFeaDim: 128

#### path
path:
#  pretrain_model_G: /home/zyyue/RetinexNet_PyTorch/ckpts/Decom/2024-06-01 12:26:00/6000.pth
  strict_load: true
  resume_state: auto

#### training settings: learning rate scheme, loss
train:
  manual_seed: 10
  lr_G: !!float 2e-4 # normalizing flow 5e-4; l1 loss train 5e-5
  weight_decay_G: 0 # 1e-5 # 5e-5 # 1e-5
  beta1: 0.9
  beta2: 0.99
#  lr_scheme: MultiStepLR
  lr_scheme: CosineAnnealingRestartCyclicLR
  warmup_iter: 10  # no warm up
#  lr_steps_rel: [ 0.5, 0.75, 0.9, 0.95 ] # [0.2, 0.35, 0.5, 0.65, 0.8, 0.95] #[0.2, 0.35, 0.5, 0.65, 0.8, 0.95]  # [ 0.5, 0.75, 0.9, 0.95 ]
#  lr_gamma: 0.5
  scheduler_CosineAnnealingRestartCyclicLR:
    periods: [ 18400, 41600 ]
    restart_weights: [ 1,1 ]
    eta_mins: [ 0.0003,0.000001 ]

  weight_l1: 0
#  flow_warm_up_iter: -1
  weight_fl: 0.4  # nll_loss weight
  weight_ll: 0.6 # loss_Decom weight
  weight_Retinex: # loss_Decom_elements
    recon_loss_low: 1
    recon_loss_high: 1
    recon_loss_mutal_low: 0.001
    recon_loss_mutal_high: 0.001
    Ismooth_loss_low: 0.1
    Ismooth_loss_high: 0.1
    equal_R_loss: 0.5
    color_loss: 0.1

  niter: 60000 #200000
  val_freq: 200

#### validation settings
val:
  # heats: [ 0.0, 0.5, 0.75, 1.0 ]
  n_sample: 4

test:
  heats: [ 0.0, 0.7, 0.8, 0.9 ]

#### logger
logger:
  # Debug print_freq: 100
  print_freq: 200
  save_checkpoint_freq: !!float 1e3
